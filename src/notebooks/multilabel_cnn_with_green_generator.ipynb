{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ead546938f820c639da19efd15421b9ad03e3307"
   },
   "source": [
    "# Imports and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "34fc840f0ef799ac31c5a75bf5b5bf8ceb155a0e"
   },
   "source": [
    "## loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import gc\n",
    "import subprocess\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Linear algebra and ML tools\n",
    "import numpy as np\n",
    "import scipy as scp\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", palette='husl')\n",
    "plt.rcParams['legend.framealpha'] = 1\n",
    "plt.rcParams['legend.frameon'] = True\n",
    "plt.rcParams['legend.edgecolor'] = 'k'\n",
    "plt.rcParams['legend.fancybox'] = False\n",
    "plt.rcParams['legend.facecolor'] = 'w'\n",
    "plt.rcParams['legend.handlelength'] = 1\n",
    "plt.rcParams['legend.handleheight'] = 1.125\n",
    "plt.rcParams['figure.figsize'] = (16, 9)\n",
    "\n",
    "INPUT_PATH = '../input/'\n",
    "TRAIN_PATH = INPUT_PATH + 'train/'\n",
    "%load_ext memory_profiler\n",
    "\n",
    "label_mapping = {\n",
    "    'class_0': 'Nucleoplasm',\n",
    "    'class_1': 'Nuclear membrane',\n",
    "    'class_2': 'Nucleoli',\n",
    "    'class_3': 'Nucleoli fibrillar center',\n",
    "    'class_4': 'Nuclear speckles',\n",
    "    'class_5': 'Nuclear bodies',\n",
    "    'class_6': 'Endoplasmic reticulum',\n",
    "    'class_7': 'Golgi apparatus',\n",
    "    'class_8': 'Peroxisomes',\n",
    "    'class_9': 'Endosomes',\n",
    "    'class_10': 'Lysosomes',\n",
    "    'class_11': 'Intermediate filaments',\n",
    "    'class_12': 'Actin filaments',\n",
    "    'class_13': 'Focal adhesion sites',\n",
    "    'class_14': 'Microtubules',\n",
    "    'class_15': 'Microtubule ends',\n",
    "    'class_16': 'Cytokinetic bridge',\n",
    "    'class_17': 'Mitotic spindle',\n",
    "    'class_18': 'Microtubule organizing center',\n",
    "    'class_19': 'Centrosome',\n",
    "    'class_20': 'Lipid droplets',\n",
    "    'class_21': 'Plasma membrane',\n",
    "    'class_22': 'Cell junctions',\n",
    "    'class_23': 'Mitochondria',\n",
    "    'class_24': 'Aggresome',\n",
    "    'class_25': 'Cytosol',\n",
    "    'class_26': 'Cytoplasmic bodies',\n",
    "    'class_27': 'Rods & rings',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe37ea9712a764d0b7f055f987aa30b9080f380e"
   },
   "source": [
    "## loading train labels and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e346a0e4088071e7f7d2820682754e6fcdeb47c"
   },
   "outputs": [],
   "source": [
    "n_classes = len(label_mapping)\n",
    "train_labels = pd.read_csv(INPUT_PATH + 'train.csv')\n",
    "train_labels['Target'] = train_labels['Target'].apply(lambda x: list(map(int, x.split(' '))))\n",
    "\n",
    "def vectorize(arr, size=n_classes):\n",
    "    res = np.zeros(size, dtype=int)\n",
    "    res[arr] = 1\n",
    "    return res\n",
    "\n",
    "train_labels['target_v'] = train_labels['Target'].apply(vectorize)\n",
    "train_labels[list(label_mapping.keys())] = pd.DataFrame(train_labels['target_v'].values.tolist(), index=train_labels.index)\n",
    "train_labels.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8fbc4d79e5d95ac1d5774e9e46a0b6ea5fb0069c"
   },
   "source": [
    "### green channel only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ae3d9c15016fd3325606bd5a60a758854a8912d"
   },
   "outputs": [],
   "source": [
    "# working with green channel only\n",
    "train_labels['Id'] = train_labels['Id'] + '_green.png'\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "31f6a42a6a8e70d54b1b81fa823fea4617bf670e"
   },
   "source": [
    "## class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "41bed88553f6350d9ac6637da31c3f82701249f5"
   },
   "outputs": [],
   "source": [
    "class_count = train_labels['target_v'].sum()\n",
    "pd.Series(class_count).plot('bar', width=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "289ea9fcc5f388d8834fd9ce21cde6ae0a223311"
   },
   "source": [
    "## Nb labels distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aec1b812a585b9cc56d96e40201ca668cff6dce2"
   },
   "outputs": [],
   "source": [
    "label_count = train_labels['target_v'].apply(sum)\n",
    "label_count.value_counts().plot('bar', width=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "63112d799baf80985de43bf4985c7b0fc59dc102"
   },
   "source": [
    "## StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b6693b9d8a86a59842cfb7bcd823eb9984965f4"
   },
   "source": [
    "### coalesce target to stratify on under-represented classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0ab7bf742a424a7e0236ba39adc5d2e4c1a9ddfa"
   },
   "outputs": [],
   "source": [
    "def coalesce(arr):\n",
    "    return arr[np.argmin(np.array(class_count)[arr])]\n",
    "    \n",
    "train_labels['y_coal'] = train_labels['Target'].apply(coalesce)\n",
    "train_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "03d5b46be97948547e5657a42ec34fa7c5dbb469"
   },
   "outputs": [],
   "source": [
    "train_labels['y_coal'].value_counts().sort_index().plot('bar', width=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c68f07924f8947076462f57c3f1f5be23afaa34"
   },
   "source": [
    "### StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fedcd1c73d0f6eb1f97e36c8790918df423f8e8d"
   },
   "source": [
    "### if train/test/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c192ab7054208891bf30db38a7747b75be500146"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "# for train_index, test_index in sss.split(np.zeros(len(train_labels)), train_labels['y_coal']):\n",
    "#     df_train = train_labels.iloc[train_index]\n",
    "#     df_test = train_labels.iloc[test_index]\n",
    "\n",
    "# for train_index, val_index in sss.split(np.zeros(len(df_train)), df_train['y_coal']):\n",
    "#     df_val = df_train.iloc[val_index]\n",
    "#     df_train = df_train.iloc[train_index]\n",
    "\n",
    "# # reset_index\n",
    "# df_train.reset_index(drop=True, inplace=True)\n",
    "# df_val.reset_index(drop=True, inplace=True)\n",
    "# df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print('nb_train: {} / class_count: {}'.format(len(df_train), df_train.target_v.sum()))\n",
    "# print('nb_val: {} / class_count: {}'.format(len(df_val), df_val.target_v.sum()))\n",
    "# print('nb_test: {} / class_prop: {}'.format(len(df_test), df_test.target_v.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3631fe96a03eb978aed85915e0a8e6632c390793"
   },
   "source": [
    "### if train/val only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1ca224d1c2d0d86c80ec62efe8ad34ddbe759608"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "for train_index, val_index in sss.split(np.zeros(len(train_labels)), train_labels['y_coal']):\n",
    "    df_train = train_labels.iloc[train_index]\n",
    "    df_val = train_labels.iloc[val_index]\n",
    "\n",
    "# reset_index\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('nb_train: {} / class_count: {}'.format(len(df_train), df_train.target_v.sum()))\n",
    "print('nb_val: {} / class_count: {}'.format(len(df_val), df_val.target_v.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c41d48d05cc9eeb661f825b03dde339c0f95288"
   },
   "outputs": [],
   "source": [
    "# some gc collection\n",
    "del train_labels\n",
    "import gc\n",
    "gc.enable()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "edbf0ab7873c4e279756a6b5c7dcf85dfe048f7d"
   },
   "source": [
    "# Running CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "adc5409bf62a0662ef06b534dbfda43106d05e50"
   },
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a077f7ade9f7ca25962ab9ab9d2d79154f8d0351"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2a764c779d4d4df1cfa97adb3901cc903852f08b"
   },
   "source": [
    "## DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6aac29bf6d2ca29557f2c28dd33b93ac97a23016"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size_train = 64\n",
    "batch_size_val = 64\n",
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "channels = 1\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(dataframe=df_train, directory=TRAIN_PATH, x_col=\"Id\", y_col=list(label_mapping.keys()), color_mode='grayscale',\n",
    "                                              class_mode='other', target_size=(WIDTH,HEIGHT), batch_size=batch_size_train)\n",
    "val_generator = datagen.flow_from_dataframe(dataframe=df_val, directory=TRAIN_PATH, x_col=\"Id\", y_col=list(label_mapping.keys()), color_mode='grayscale',\n",
    "                                            class_mode=\"other\", target_size=(WIDTH,HEIGHT), batch_size=batch_size_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0cafbb1193a7eac19d7282395d8254e0457907cc"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f1185b2c4e6023937552f47e151d120678d4fa1d"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "filters0 = 8\n",
    "nb_dense = 64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters0, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(WIDTH, HEIGHT, channels)))\n",
    "model.add(Conv2D(filters0, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(WIDTH, HEIGHT, channels)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters0 * 2, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters0 * 2, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters0 * 4, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters0 * 4, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters0 * 4, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters0 * 4, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters0 * 8, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters0 * 8, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters0 * 8, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters0 * 16, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters0 * 16, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters0 * 16, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(nb_dense, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d46c8e88611091ef8c559f86af674c932a7dae80"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size\n",
    "STEP_SIZE_VALID = val_generator.n // val_generator.batch_size\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.0001\n",
    "callback= EarlyStopping(monitor='val_loss', patience=40)\n",
    "\n",
    "model.compile(loss=f1_loss,\n",
    "              optimizer=Adam(lr=learning_rate),\n",
    "              metrics=[f1])\n",
    "\n",
    "output = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=STEP_SIZE_VALID,\n",
    "    callbacks=[callback],\n",
    "    use_multiprocessing=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aab41bbd30737e9a087ae1c0c5d8939d13210de6"
   },
   "outputs": [],
   "source": [
    "model.save_weights('multilabel_cnn_green.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f6b2070d211dec69f12630f26cdd2a36b97913dc"
   },
   "source": [
    "## Result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe23b03cce4e0717941601603bdef4e9f827b239"
   },
   "outputs": [],
   "source": [
    "# Plotting the performance of our network\n",
    "plt.plot(output.epoch, output.history['loss'], label='train')\n",
    "plt.plot(output.epoch, output.history['val_loss'], label='val')\n",
    "plt.title('Training and validation performance')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a48f08d03f17d4cc6dfc85711cb4ab1cd1d9f7a3"
   },
   "source": [
    "# Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1a3b3add7b8f3e79b416dc7636b23fe0b64c9f8"
   },
   "outputs": [],
   "source": [
    "# y_predict_proba = model.predict()\n",
    "# f1_score(y_train, y_predict_proba.round(), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec3b68c49d1b282174e068dd98cdba4fd1697b67"
   },
   "outputs": [],
   "source": [
    "# diff = y_val != y_predict_proba.round()\n",
    "# x_test_errors = X_val[diff]\n",
    "# y_test_errors = y_val_class[diff]\n",
    "# y_predict_errors = y_predict[diff]\n",
    "# y_predict_proba_errors = y_predict_proba[diff]\n",
    "\n",
    "# print('{} errors out of {}'.format(len(x_val_errors), len(X_val)))\n",
    "\n",
    "# index = np.random.randint(len(y_predict_errors))\n",
    "\n",
    "# print(\"Correct label is: \", y_val_errors[index])\n",
    "# print(\"Predicted label is: \", y_predict_errors[index])\n",
    "# print(\"Probabilities: \", y_predict_proba_errors[index])\n",
    "# plt.imshow(np.squeeze(x_test_errors[index]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
